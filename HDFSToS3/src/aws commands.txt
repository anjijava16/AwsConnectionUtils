i) Create new Bucket

ii)Bucket Name: nyse 
  Region:   US
iii)Bucketname:nyse_iwinner

iii)  Inside the our Bucket upload the data

iv) export AWS_SECURET_ACCESS_KEY=
    export AWS_ACCESS_KEY

V) aws s3 ls 
  it's contains 3 buckets

vi) aws s3 ls itest

vii) aws s3 ls s3://itest/jar

viii)aws s3 ls s3://itest/jar

ix) aws s3 sync ~/localdata/NYSE-2009 s3://itest/abac_2017

x)hadoop distcp -Dfs.s3a.awsAccessKeyId="jsjjfs"   
-Dfs.s3a.awsSecretAccessKey="U" -delete -overwrite
 s3a://kkaka/fskkf/  /kakkfd/lls


hadoop distcp -Dfs.s3a.access.key=myAccessKey -Dfs.s3a.secret.key=mySecretKey hdfs://user/hdfs/mydata s3a://myBucket/mydata_backup

xi) S3 TO HDFS 
hadoop jar /etc/hadoop-2.2.0/share/hadoop/tools/lib/s3distcp.jar --src s3n://$YOUR_S3_BUCKET/$S3_SOURCE_DIR --dest $HDFS_SOURCE_DIR

xii) HDFS to S3
hadoop jar /etc/hadoop-2.2.0/share/hadoop/tools/lib/s3distcp.jar --src $HDFS_TARGET_DIR --dest s3n://$YOUR_S3_BUCKET/$S3_TARGET_DIR

xiii)Till now i was using multi-part upload to s3 using s3’s command line options. So uploading files larger than 5gb to s3 from my local system was not a problem. But this week i wanted to upload large files from hdfs to s3. Till now all data transfers from hdfs to s3 was done to through distcp. But for this case even distcp will fail because the file’s size was more than 5gb. One way to do it was to copy the file from hdfs to my local system and then upload it using multi-part. But that wasn’t cool.


